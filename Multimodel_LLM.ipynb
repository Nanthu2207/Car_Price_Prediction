{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nanthu2207/Car_Price_Prediction/blob/main/Multimodel_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the libraries"
      ],
      "metadata": {
        "id": "wpaGvzfbyg90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "trEpEwu2dSv1",
        "outputId": "afe7793e-7c05-41a9-b20b-8a50bb10ed7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.7.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.0 (from gradio)\n",
            "  Downloading gradio_client-1.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.0->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.5.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.7.1-py3-none-any.whl (57.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.0-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.4.0 gradio-5.7.1 gradio-client-1.5.0 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.8.1 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.12.0 uvicorn-0.32.1 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Nk3y_SaIxH1g",
        "outputId": "0c9398f5-5f21-461c-a9a7-f300b73d7810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.13.0-py3-none-any.whl (108 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 . Model Selection**"
      ],
      "metadata": {
        "id": "qYBF9anpIG9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamil to English translation model : \"openai/whisper-large-v3\""
      ],
      "metadata": {
        "id": "Lmm6ODZ7DBBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces."
      ],
      "metadata": {
        "id": "DXfoCAI9YTQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application Development**"
      ],
      "metadata": {
        "id": "JoLsptRlIIGL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "VJ50_sY0Y6bv",
        "outputId": "6763c602-0bc6-4a90-d2d4-52f97dc67103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:1024: UserWarning: Cannot load huggingface. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/huggingface (Request ID: Root=1-67508a1b-6ec842e858175f0d19ec7103;7e85859b-7176-4488-8040-3bb19f8db099)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:1024: UserWarning: Cannot load huggingface. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/huggingface (Request ID: Root=1-67508a1c-28b0fb2a594a7f90325046bf;0ae4918b-d658-4d6b-b395-500969bbe243)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b8d1d004a83fd3af4a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b8d1d004a83fd3af4a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "MODEL_NAME = \"openai/whisper-large-v3\"\n",
        "BATCH_SIZE = 8\n",
        "FILE_LIMIT_MB = 1000\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=MODEL_NAME,\n",
        "    chunk_length_s=30,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "def transcribe(inputs, task):\n",
        "    if inputs is None:\n",
        "        raise gr.Error(\"No audio file submitted! Please upload or record an audio file before submitting your request.\")\n",
        "\n",
        "    text = pipe(inputs, batch_size=BATCH_SIZE, generate_kwargs={\"task\": task}, return_timestamps=True)[\"text\"]\n",
        "    return  text\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "mf_transcribe = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
        "        gr.Radio([\"translate\"], label=\"Task\", value=\"translate\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    theme=\"huggingface\",\n",
        "    title=\"Whisper Large V3: Translate Audio\",\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "file_transcribe = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.Audio(sources=\"upload\", type=\"filepath\",  label=\"Audio file\"),\n",
        "        gr.Radio([\"translate\"], label=\"Task\", value=\"translate\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    theme=\"huggingface\",\n",
        "    title=\"Whisper Large V3: Translate Audio\",\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "with demo:\n",
        "    gr.TabbedInterface([mf_transcribe, file_transcribe], [\"Microphone\", \"Audio file\"])\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **2 . Model Selection**"
      ],
      "metadata": {
        "id": "qLvr1rzKHjoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model : Image Generation with Flux-RealismLora"
      ],
      "metadata": {
        "id": "LZGaboDMDaS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text-to-Image :\n",
        "Text-to-image is the task of generating images from input text.\n",
        "These pipelines can also be used to modify and edit images based on text prompts."
      ],
      "metadata": {
        "id": "RVxnMu_wXUXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application Development**"
      ],
      "metadata": {
        "id": "02VFcboXXiSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/XLabs-AI/flux-RealismLora\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"} #Hugging Face API token\n",
        "\n",
        "def query(prompt):\n",
        "    payload = {\"inputs\": prompt}\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    image_bytes = response.content\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "    return image\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=query,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your image generation prompt here...\"),\n",
        "    outputs=\"image\",\n",
        "    title=\"Image Generation with Flux-RealismLora\",\n",
        ")\n",
        "\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "6UUhbRNlDb3e",
        "outputId": "2030c41f-f673-47e1-8080-beea98cd7a35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://82028fc5345b222779.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://82028fc5345b222779.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **3 . Model Selection**"
      ],
      "metadata": {
        "id": "TnfQIvbcGwX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model : GPT-Neo"
      ],
      "metadata": {
        "id": "iGgiUgTeGwDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating text is the task of generating new text given another text. These models can, for example, fill in incomplete text or paraphrase."
      ],
      "metadata": {
        "id": "OoDDQv1VqDZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application Developmen**"
      ],
      "metadata": {
        "id": "-uw1REI0Gr-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n",
        "# Define the text generation function\n",
        "def generate_text(prompt, temperature=0.9, max_length=100):\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text with attention_mask and pad_token_id\n",
        "    gen_tokens = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
        "    return gen_text\n",
        "\n",
        "# Create the Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter your prompt:\", placeholder=\"Start typing...\"),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Text\"),\n",
        "    title=\"GPT-Neo Text Generator\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "93Mt5KScB53E",
        "outputId": "22268c7e-c269-4294-b2fa-4cb0008f3136"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4285ac3656711f1b56.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4285ac3656711f1b56.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline, GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Model and device configuration for Whisper\n",
        "MODEL_NAME = \"openai/whisper-large-v3\"\n",
        "BATCH_SIZE = 8\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize Whisper pipeline\n",
        "pipe = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=MODEL_NAME,\n",
        "    chunk_length_s=30,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Define transcription function\n",
        "def transcribe(inputs, task):\n",
        "    if inputs is None:\n",
        "        raise gr.Error(\"No audio file submitted! Please upload or record an audio file before submitting your request.\")\n",
        "    result = pipe(inputs, batch_size=BATCH_SIZE, generate_kwargs={\"task\": task}, return_timestamps=True)\n",
        "    return result[\"text\"]\n",
        "\n",
        "# API details for image generation\n",
        "API_URL = \"https://api-inference.huggingface.co/models/XLabs-AI/flux-RealismLora\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}  # Replace with your Hugging Face token\n",
        "\n",
        "# Define image generation function\n",
        "def generate_image(prompt):\n",
        "    payload = {\"inputs\": prompt}\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    image_bytes = response.content\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "    return image\n",
        "\n",
        "# Initialize GPT-Neo model and tokenizer\n",
        "text_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "text_tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n",
        "# Define text generation function\n",
        "def generate_text(prompt, temperature=0.9, max_length=100):\n",
        "    inputs = text_tokenizer(prompt, return_tensors=\"pt\")\n",
        "    gen_tokens = text_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=text_tokenizer.eos_token_id,\n",
        "    )\n",
        "    gen_text = text_tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
        "    return gen_text\n",
        "\n",
        "# Gradio app with multiple functionalities in tabs\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Tab(\"Microphone Transcription\"):\n",
        "        gr.Markdown(\"### Whisper Large V3: Microphone Transcription\")\n",
        "        gr.Markdown(\n",
        "            \"Transcribe long-form microphone inputs with the click of a button! This demo uses OpenAI Whisper to process microphone inputs.\"\n",
        "        )\n",
        "        mic_input = gr.Audio(sources=\"microphone\", type=\"filepath\", label=\"Microphone Input\")\n",
        "        mic_task = gr.Radio([\"translate\"], label=\"Task\", value=\"translate\")\n",
        "        mic_output = gr.Textbox(label=\"Transcribed Text\")\n",
        "        gr.Button(\"Submit\").click(transcribe, inputs=[mic_input, mic_task], outputs=mic_output)\n",
        "\n",
        "    with gr.Tab(\"File Upload Transcription\"):\n",
        "        # gr.Markdown(\"### Whisper Large V3: File Transcription\")\n",
        "        # gr.Markdown(\n",
        "        #     \"Transcribe long-form audio files with the click of a button! This demo uses OpenAI Whisper to process uploaded audio files.\"\n",
        "        # )\n",
        "        file_input = gr.Audio(sources=\"upload\", type=\"filepath\", label=\"Upload Audio File\")\n",
        "        file_task = gr.Radio([\"translate\"], label=\"Task\", value=\"translate\")\n",
        "        file_output = gr.Textbox(label=\"Transcribed Text\")\n",
        "        gr.Button(\"Submit\").click(transcribe, inputs=[file_input, file_task], outputs=file_output)\n",
        "\n",
        "    with gr.Tab(\"Image Generation\"):\n",
        "        # gr.Markdown(\"### Image Generation with Flux-RealismLora\")\n",
        "        # gr.Markdown(\n",
        "        #     \"Generate images from text prompts using the Flux-RealismLora model on Hugging Face.\"\n",
        "        # )\n",
        "        img_prompt = gr.Textbox(lines=2, placeholder=\" image\")\n",
        "        img_output = gr.Image(label=\"Generated Image\")\n",
        "        gr.Button(\"Generate\").click(generate_image, inputs=img_prompt, outputs=img_output)\n",
        "\n",
        "    with gr.Tab(\"Text Generation\"):\n",
        "        gr.Markdown(\"### GPT-Neo Text Generator\")\n",
        "        gr.Markdown(\n",
        "            \"Generate text using GPT-Neo. Adjust the temperature for creativity and max length for output size.\"\n",
        "        )\n",
        "        txt_prompt = gr.Textbox(label=\"mic_output \", placeholder\"=outputs\")\n",
        "        txt_temperature = gr.Slider(0.1, 1.0, value=0.9, step=0.1, label=\"Temperature\")\n",
        "        txt_max_length = gr.Slider(10, 200, value=100, step=10, label=\"Max Length\")\n",
        "        txt_output = gr.Textbox(label=\"Generated Text\")\n",
        "        gr.Button(\"Generate\").click(\n",
        "            generate_text, inputs=[txt_prompt, txt_temperature, txt_max_length], outputs=txt_output\n",
        "        )\n",
        "\n",
        "# Launch the app\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "68Oo5_6HRaXw",
        "outputId": "703ecfb8-01ca-41a2-eb12-312f5ea67195"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'outputs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-633bd06c8f3b>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;34m\"Generate images from text prompts using the Flux-RealismLora model on Hugging Face.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         ) \n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mimg_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mimg_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generated Image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mButton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application Development"
      ],
      "metadata": {
        "id": "2cefHSnuOHbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline, GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Model and device configuration for Whisper\n",
        "MODEL_NAME = \"openai/whisper-large-v3\"\n",
        "BATCH_SIZE = 8\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize Whisper pipeline\n",
        "pipe = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=MODEL_NAME,\n",
        "    chunk_length_s=30,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Define translation function\n",
        "def translate(inputs, task):\n",
        "    if inputs is None:\n",
        "        raise gr.Error(\"No audio file submitted! Please upload or record an audio file before submitting your request.\")\n",
        "    result = pipe(inputs, batch_size=BATCH_SIZE, generate_kwargs={\"task\": task}, return_timestamps=True)\n",
        "    return result[\"text\"]\n",
        "\n",
        "# API details for image generation\n",
        "API_URL = \"https://api-inference.huggingface.co/models/XLabs-AI/flux-RealismLora\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}  # Replace with your Hugging Face token\n",
        "\n",
        "# Define image generation function\n",
        "def generate_image(prompt):\n",
        "    payload = {\"inputs\": prompt}\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    image_bytes = response.content\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "    return image\n",
        "\n",
        "# Initialize GPT-Neo model and tokenizer\n",
        "text_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "text_tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n",
        "# Define text generation function\n",
        "def generate_text(prompt, temperature=0.9, max_length=100):\n",
        "    inputs = text_tokenizer(prompt, return_tensors=\"pt\")\n",
        "    gen_tokens = text_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=text_tokenizer.eos_token_id,\n",
        "    )\n",
        "    gen_text = text_tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
        "    return gen_text\n",
        "\n",
        "# Gradio app with multiple functionalities in tabs\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Tab(\"Microphone Transcription\"):\n",
        "        gr.Markdown(\"Audio Translation\")\n",
        "        mic_input = gr.Audio(sources=\"microphone\", type=\"filepath\", label=\"Microphone Input\")\n",
        "        mic_task = gr.Radio([\"translate\"], label=\"Task\", value=\"translate\")\n",
        "        mic_output = gr.Textbox(label=\"Translated Text\")\n",
        "        gr.Button(\"Submit\").click(translate, inputs=[mic_input, mic_task], outputs=mic_output)\n",
        "        gr.Markdown(\"Image Generation \")\n",
        "        img_output = gr.Image(label=\"Generated Image\")\n",
        "        gr.Button(\"Generate\").click(generate_image, inputs=mic_output, outputs=img_output)\n",
        "        gr.Markdown(\" Text Generaion\")\n",
        "\n",
        "        txt_output = gr.Textbox(label=\"Generated Text\")\n",
        "        gr.Button(\"Generate\").click(\n",
        "            generate_text, inputs=[mic_output], outputs=txt_output\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"File Upload Translation\"):\n",
        "        gr.Markdown(\"File Translation\")\n",
        "        file_input = gr.Audio(sources=\"upload\", type=\"filepath\", label=\"Upload Audio File\")\n",
        "        file_task = gr.Radio([\"translate\"], label=\"Task\", value=\"translate\")\n",
        "        file_output = gr.Textbox(label=\"Translated Text\")\n",
        "        gr.Button(\"Submit\").click(translate, inputs=[file_input, file_task], outputs=file_output)\n",
        "        gr.Markdown(\" Image Generation\")\n",
        "        img_output = gr.Image(label=\"Generated Image\")\n",
        "        gr.Button(\"Generate\").click(generate_image, inputs=file_output, outputs=img_output)\n",
        "        gr.Markdown(\"### GPT-Neo Text Generator\")\n",
        "\n",
        "        txt_output = gr.Textbox(label=\"Generated Text\")\n",
        "        gr.Button(\"Generate\").click(\n",
        "            generate_text, inputs=[file_output], outputs=txt_output\n",
        "        )\n",
        "\n",
        "\n",
        "# Launch the app\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "25b80c1a-5306-4e10-d5f6-794ed6f76a66",
        "id": "wltWW5rtFIWT"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c32d9c40451fec7094.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c32d9c40451fec7094.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1jjb+ahKIOM+NlWyymYP3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}